# Доп. материалы к разделам курса [«Введение в машинное обучение»](https://www.coursera.org/learn/vvedenie-mashinnoe-obuchenie)

## Логические методы: решающие деревья и решающие леса

* Урок про [Desicion Trees](http://scikit-learn.org/stable/modules/tree.html) на scikit-learn.org;
* [Логические алгоритмы классификации](http://www.machinelearning.ru/wiki/images/9/97/Voron-ML-Logic-slides.pdf) – К. В. Воронцов;
* [Семинары по решающим деревьям](https://github.com/esokolov/ml-course-msu/blob/master/ML15/lecture-notes/Sem04_trees.pdf) – Е. Соколов.

## Grid Search

 * [How to Tune Algorithm Parameters with Scikit-Learn](http://machinelearningmastery.com/how-to-tune-algorithm-parameters-with-scikit-learn/);
 * [Python and Kaggle: Feature selection, multiple models and Grid Search](http://miguelmalvarez.com/2015/02/23/python-and-kaggle-feature-selection-multiple-models-and-grid-search/);
 * [Grid search and cross-validated estimators](http://www.scipy-lectures.org/packages/scikit-learn/#grid-search-and-cross-validated-estimators) на scipy-lectures.org;
 * Документация по модулю [Grid Search](http://scikit-learn.org/stable/modules/grid_search.html#grid-search) на scikit-learn.org

## Логистическая регрессия

 * [Подробнее о логистической регрессии и предсказании вероятностей с ее помощью](https://github.com/esokolov/ml-course-msu/blob/master/ML15/lecture-notes/Sem10_linear.pdf);
 * [Семинары по выбору моделей и критериев качества](https://github.com/esokolov/ml-course-msu/blob/master/ML15/lecture-notes/Sem05_metrics.pdf).

## Градиентный спуск

 * [Подробнее о градиентах и градиентном спуске](https://github.com/esokolov/ml-course-msu/blob/master/ML15/lecture-notes/Sem07_linear.pdf);
 * [Методичка по методам спуска и градиентам](http://www.apmath.spbu.ru/ru/staff/grigorieva/mfk.pdf);
 * [Объяснение градиента (видео)](https://www.youtube.com/watch?v=qlLChbHhbg4&feature=youtu.be&list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC&t=50m16s) от Andrej Karpathy, отрывок из лекции курса CS231n.

## Градиентный бустинг

* [Подробнее о градиентном бустинге и особенностях его применения к деревьям](http://www.machinelearning.ru/wiki/images/7/7e/Sem03_ensembles_2014.pdf);
* [Ещё о выводе градиентного бустинга для регрессии и классификации](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf).

## Кластеризация

 * [Soft K-means](http://cs.gmu.edu/~kosecka/cs803/soft-kmeans.pdf) – коротко, строго и одновременно понятно.

## Метод опорных векторов (SVM)

 * [Вывод SVM](http://cs229.stanford.edu/notes/cs229-notes3.pdf) - Заметки оригинального стэнфордского курса Andrew Ng;
 * [Метод опорных векторов](http://statistica.ru/local-portals/data-mining/metod-opornykh-vektorov/).

## Нейронные сети

* [Лекции по искусственным нейронным сетям](http://www.ccas.ru/voron/download/NeuralNets.pdf) — К. В. Воронцов;
* [Deep Learning](http://www.deeplearningbook.org) - Ian Goodfellow, Yoshua Bengio, and Aaron Courville (2016);
* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html) – ещё одна бесплатная онлайн-книга.
